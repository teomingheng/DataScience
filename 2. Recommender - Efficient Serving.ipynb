{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff97d21e-6a72-4c47-b1a1-4452398cd78b",
   "metadata": {},
   "source": [
    "# 2. Recommendation Model: Efficient Serving using approximate nearest neighbour search (ANN) \n",
    "\n",
    "https://www.tensorflow.org/recommenders/examples/efficient_serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6e93a6-7cf0-4d1c-b3de-7a0109f7b6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q tensorflow-recommenders\n",
    "%pip install -q --upgrade tensorflow-datasets\n",
    "%pip install -q scann\n",
    "%pip install -q tf_keras # install legacy keras to overcome bug in https://github.com/tensorflow/recommenders/issues/712 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ef866f-841f-4873-aa15-d8b0f787f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1' \n",
    "\n",
    "import mlflow\n",
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92cb24b9-7f34-4a3f-99ab-05ef09c4c514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 16:21:19.076381: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-12 16:21:19.087004: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-12 16:21:19.090206: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-12 16:21:19.098971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-12 16:21:19.633696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024/10/12 16:21:19 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7118e-2729-447b-b690-484b7f84c098",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab242db2-74ff-4f25-82d4-fe223dea053c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728721280.164800   10468 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728721280.195405   10468 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728721280.195612   10468 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728721280.196666   10468 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728721280.196828   10468 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728721280.196974   10468 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728721280.254268   10468 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728721280.254433   10468 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728721280.254573   10468 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-12 16:21:20.254688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Load the MovieLens 100K data.\n",
    "ratings = tfds.load(\n",
    "    \"movielens/100k-ratings\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "# Get the ratings data.\n",
    "ratings = (ratings\n",
    "           # Retain only the fields we need.\n",
    "           .map(lambda x: {\"user_id\": x[\"user_id\"], \"movie_title\": x[\"movie_title\"]})\n",
    "           # Cache for efficiency.\n",
    "           .cache(tempfile.NamedTemporaryFile().name)\n",
    ")\n",
    "\n",
    "# Get the movies data.\n",
    "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
    "movies = (movies\n",
    "          # Retain only the fields we need.\n",
    "          .map(lambda x: x[\"movie_title\"])\n",
    "          # Cache for efficiency.\n",
    "          .cache(tempfile.NamedTemporaryFile().name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdebf124-f46d-4fb4-8e28-6cdc9afb4f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 16:21:20.496457: W tensorflow/core/kernels/data/cache_dataset_ops.cc:332] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-10-12 16:21:20.497699: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-10-12 16:21:21.529405: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-10-12 16:21:21.529723: W tensorflow/core/kernels/data/cache_dataset_ops.cc:332] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "user_ids = ratings.map(lambda x: x[\"user_id\"])\n",
    "\n",
    "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids.batch(1000))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad4473-ac86-4ae5-baaf-54addf126e16",
   "metadata": {},
   "source": [
    "# Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c6eb33-acc8-47f5-ba67-adb41a03ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(80_000)\n",
    "test = shuffled.skip(80_000).take(20_000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a662e-1b73-4a59-9941-052a9922592b",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "652947ea-50da-47f0-b260-c0401c1e3645",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    embedding_dimension = 32\n",
    "\n",
    "    # Set up a model for representing movies.\n",
    "    self.movie_model = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_movie_titles, mask_token=None),\n",
    "      # We add an additional embedding to account for unknown tokens.\n",
    "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Set up a model for representing users.\n",
    "    self.user_model = tf.keras.Sequential([\n",
    "      tf.keras.layers.StringLookup(\n",
    "        vocabulary=unique_user_ids, mask_token=None),\n",
    "        # We add an additional embedding to account for unknown tokens.\n",
    "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Set up a task to optimize the model and compute metrics.\n",
    "    self.task = tfrs.tasks.Retrieval(\n",
    "      metrics=tfrs.metrics.FactorizedTopK(\n",
    "        candidates=(\n",
    "            movies\n",
    "            .batch(128)\n",
    "            .cache()\n",
    "            .map(lambda title: (title, self.movie_model(title)))\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    # We pick out the user features and pass them into the user model.\n",
    "    user_embeddings = self.user_model(features[\"user_id\"])\n",
    "    # And pick out the movie features and pass them into the movie model,\n",
    "    # getting embeddings back.\n",
    "    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "\n",
    "    return self.task(\n",
    "        user_embeddings,\n",
    "        positive_movie_embeddings,\n",
    "        candidate_ids=features[\"movie_title\"],\n",
    "        compute_metrics=not training\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13e0f2-cfbb-423f-aefa-e8089c120f12",
   "metadata": {},
   "source": [
    "# Fitting & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3396f8-dd43-4337-934e-db7f578e9063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 16:21:22 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: The layer \"movielens_model\" has never been called and thus has no defined input shape. Note that the `input_shape` property is only available for Functional and Sequential models.\n",
      "2024/10/12 16:21:23 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728721283.763809   10631 service.cc:146] XLA service 0x74c2c46ef680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1728721283.763829   10631 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2024-10-12 16:21:23.767657: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-12 16:21:23.775461: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "I0000 00:00:1728721283.802911   10631 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024/10/12 16:21:24 WARNING mlflow.utils.checkpoint_utils: Checkpoint logging is skipped, because checkpoint 'save_best_only' config is True, it requires to compare the monitored metric value, but the provided monitored metric value is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 1s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 53547.8438 - regularization_loss: 0.0000e+00 - total_loss: 53547.8438 - 1s/epoch - 102ms/step\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 16:21:24 WARNING mlflow.utils.checkpoint_utils: Checkpoint logging is skipped, because checkpoint 'save_best_only' config is True, it requires to compare the monitored metric value, but the provided monitored metric value is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 52130.3320 - regularization_loss: 0.0000e+00 - total_loss: 52130.3320 - 365ms/epoch - 36ms/step\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 16:21:24 WARNING mlflow.utils.checkpoint_utils: Checkpoint logging is skipped, because checkpoint 'save_best_only' config is True, it requires to compare the monitored metric value, but the provided monitored metric value is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 51503.8203 - regularization_loss: 0.0000e+00 - total_loss: 51503.8203 - 359ms/epoch - 36ms/step\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 16:21:25 WARNING mlflow.utils.checkpoint_utils: Checkpoint logging is skipped, because checkpoint 'save_best_only' config is True, it requires to compare the monitored metric value, but the provided monitored metric value is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 51076.5977 - regularization_loss: 0.0000e+00 - total_loss: 51076.5977 - 362ms/epoch - 36ms/step\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 16:21:25 WARNING mlflow.utils.checkpoint_utils: Checkpoint logging is skipped, because checkpoint 'save_best_only' config is True, it requires to compare the monitored metric value, but the provided monitored metric value is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 50746.5938 - regularization_loss: 0.0000e+00 - total_loss: 50746.5938 - 364ms/epoch - 36ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 16:21:25.900161: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024/10/12 16:21:25 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: Cannot log input example or model signature for input with type <class 'tensorflow.python.data.ops.batch_op._BatchDataset'>. TensorFlow Keras autologging can only log input examples and model signatures for the following input types: numpy.ndarray, dict[string -> numpy.ndarray], tensorflow.keras.utils.Sequence, and tensorflow.data.Dataset (TensorFlow >= 2.1.0 required)\n",
      "2024/10/12 16:21:25 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2024/10/12 16:21:25 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/home/mingheng/.local/share/virtualenvs/DataScience-Eph6v9Sl/lib/python3.10/site-packages/tf_keras/src/saving/saving_api.py:164: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\"\n",
      "2024/10/12 16:21:25 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 5s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0010 - factorized_top_k/top_5_categorical_accuracy: 0.0070 - factorized_top_k/top_10_categorical_accuracy: 0.0165 - factorized_top_k/top_50_categorical_accuracy: 0.1071 - factorized_top_k/top_100_categorical_accuracy: 0.2158 - loss: 49646.2168 - regularization_loss: 0.0000e+00 - total_loss: 49646.2168\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    # Fitting\n",
    "    model = MovielensModel()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "    model.fit(train.batch(8192), epochs=5, verbose=2)\n",
    "\n",
    "    # Evaluation\n",
    "    model.evaluate(test.batch(8192), return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff591e-42c9-45c2-a6a9-fc51980a119b",
   "metadata": {},
   "source": [
    "# Brute force prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "412f7960-304b-4547-9750-c4acf47cfec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x74c7b81aa1a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brute_force = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "brute_force.index_from_dataset(\n",
    "    movies.batch(128).map(lambda title: (title, model.movie_model(title)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db6f3d95-8f86-444e-8524-46630754f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations: [b'Rudy (1993)' b'When a Man Loves a Woman (1994)'\n",
      " b'Affair to Remember, An (1957)']\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for user 42.\n",
    "_, titles = brute_force(np.array([\"42\"]), k=3)\n",
    "\n",
    "print(f\"Top recommendations: {titles[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef2db4bf-4138-4870-b3a7-7a4682b3891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "923 μs ± 13 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _, titles = brute_force(np.array([\"42\"]), k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb32135-0a0d-4bda-a094-d23406e6e366",
   "metadata": {},
   "source": [
    "# Simulate large data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b105b4c-9187-425c-a5be-d72f0efee4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dataset of movies that's 1,000 times larger. We \n",
    "# do this by adding several million dummy movie titles to the dataset.\n",
    "lots_of_movies = tf.data.Dataset.concatenate(\n",
    "    movies.batch(4096),\n",
    "    movies.batch(4096).repeat(1_000).map(lambda x: tf.zeros_like(x))\n",
    ")\n",
    "\n",
    "# We also add lots of dummy embeddings by randomly perturbing\n",
    "# the estimated embeddings for real movies.\n",
    "lots_of_movies_embeddings = tf.data.Dataset.concatenate(\n",
    "    movies.batch(4096).map(model.movie_model),\n",
    "    movies.batch(4096).repeat(1_000)\n",
    "      .map(lambda x: model.movie_model(x))\n",
    "      .map(lambda x: x * tf.random.uniform(tf.shape(x)))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2998c-2366-4134-8ee8-eadff47b68bf",
   "metadata": {},
   "source": [
    "# Brute force index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23d65a42-6d29-447d-b233-3df497dc8123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x74c7b81d66b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brute_force_lots = tfrs.layers.factorized_top_k.BruteForce()\n",
    "brute_force_lots.index_from_dataset(\n",
    "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e4bcb-1467-46d7-904b-9d8ae9aee7c8",
   "metadata": {},
   "source": [
    "# Brute force recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94a2ce66-1b8d-4d48-a8ab-19526d9878e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations: [b'Rudy (1993)' b'When a Man Loves a Woman (1994)'\n",
      " b'Affair to Remember, An (1957)']\n",
      "1.64 ms ± 3.24 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "_, titles = brute_force_lots(model.user_model(np.array([\"42\"])), k=3)\n",
    "\n",
    "print(f\"Top recommendations: {titles[0]}\")\n",
    "\n",
    "%timeit _, titles = brute_force_lots(model.user_model(np.array([\"42\"])), k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7d5cb-5d3a-45fa-995c-d1357179cbe8",
   "metadata": {},
   "source": [
    "# Approximate recommendation using ScaNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d90409f-22f0-4017-9e56-cc6fe65b3999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 16:21:59.695398: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 100347\n",
      "2024-10-12 16:21:59.747569: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:89] PartitionerFactory ran in 52.139029ms.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.ScaNN at 0x74c7b819aad0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scann = tfrs.layers.factorized_top_k.ScaNN(\n",
    "    num_reordering_candidates=500,\n",
    "    num_leaves_to_search=30\n",
    ")\n",
    "scann.index_from_dataset(\n",
    "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a100069-d140-4a0a-99fd-3d51f4f48e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations: [b'Rudy (1993)' b'When a Man Loves a Woman (1994)'\n",
      " b'Affair to Remember, An (1957)']\n",
      "11.3 ms ± 78.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "_, titles = scann(model.user_model(np.array([\"42\"])), k=3)\n",
    "\n",
    "print(f\"Top recommendations: {titles[0]}\")\n",
    "\n",
    "%timeit _, titles = scann(model.user_model(np.array([\"42\"])), k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb129ca-8e26-44cd-a7f8-484c0c233b68",
   "metadata": {},
   "source": [
    "# Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5906a8fd-c3b4-4d6e-a143-d88c47202d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 48s, sys: 1min 32s, total: 17min 20s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "# Override the existing streaming candidate source.\n",
    "model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n",
    "    candidates=tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")\n",
    "# Need to recompile the model for the changes to take effect.\n",
    "model.compile()\n",
    "\n",
    "%time baseline_result = model.evaluate(test.batch(8192), return_dict=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aa4b4b2-ccc9-4fd0-876d-9dd529ba8a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.44 s, sys: 412 ms, total: 7.85 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "model.task.factorized_metrics = tfrs.metrics.FactorizedTopK(\n",
    "    candidates=scann\n",
    ")\n",
    "model.compile()\n",
    "\n",
    "# We can use a much bigger batch size here because ScaNN evaluation\n",
    "# is more memory efficient.\n",
    "%time scann_result = model.evaluate(test.batch(8192), return_dict=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7516dc79-2c0a-456a-9ed2-8cd004a5f954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brute force top-100 accuracy: 0.12\n",
      "ScaNN top-100 accuracy:       0.10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Brute force top-100 accuracy: {baseline_result['factorized_top_k/top_100_categorical_accuracy']:.2f}\")\n",
    "print(f\"ScaNN top-100 accuracy:       {scann_result['factorized_top_k/top_100_categorical_accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82f5044-98eb-40fe-9994-9da6a93812d1",
   "metadata": {},
   "source": [
    "# Deploying approximate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41842cfb-1d44-4192-b9fb-cd9c63446f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ConcatenateDataset element_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lots_of_movies_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9528c10a-fd97-42be-b76e-9ffafde58f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 16:24:46.714249: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2024-10-12 16:24:47.047606: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 100347\n",
      "2024-10-12 16:24:47.093281: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:89] PartitionerFactory ran in 45.643806ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpc9qqiz9n/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpc9qqiz9n/model/assets\n"
     ]
    }
   ],
   "source": [
    "# We re-index the ScaNN layer to include the user embeddings in the same model.\n",
    "# This way we can give the saved model raw features and get valid predictions\n",
    "# back.\n",
    "scann = tfrs.layers.factorized_top_k.ScaNN(model.user_model, num_reordering_candidates=1000)\n",
    "scann.index_from_dataset(\n",
    "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")\n",
    "\n",
    "# Need to call it to set the shapes.\n",
    "_ = scann(np.array([\"42\"]))\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "  path = os.path.join(tmp, \"model\")\n",
    "  tf.saved_model.save(\n",
    "      scann,\n",
    "      path,\n",
    "      options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"])\n",
    "  )\n",
    "\n",
    "  loaded = tf.saved_model.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b40b660-e1d9-4bb3-b9f9-7db5702f993c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations: [b'Rudy (1993)' b'When a Man Loves a Woman (1994)'\n",
      " b'Affair to Remember, An (1957)']\n"
     ]
    }
   ],
   "source": [
    "_, titles = loaded(tf.constant([\"42\"]))\n",
    "\n",
    "print(f\"Top recommendations: {titles[0][:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a06da2c-3443-4164-8d49-4c9e4a166b60",
   "metadata": {},
   "source": [
    "# Tuning Scann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d2e8f22-c0be-4433-91cb-5bde7b5b3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process queries in groups of 1000; processing them all at once with brute force\n",
    "# may lead to out-of-memory errors, because processing a batch of q queries against\n",
    "# a size-n dataset takes O(nq) space with brute force.\n",
    "titles_ground_truth = tf.concat([\n",
    "  brute_force_lots(queries, k=10)[1] for queries in\n",
    "  test.batch(1000).map(lambda x: model.user_model(x[\"user_id\"]))\n",
    "], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86dce9b1-18db-4e6a-bcbd-25cce2e56017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all user_id's as a 1d tensor of strings\n",
    "test_flat = np.concatenate(list(test.map(lambda x: x[\"user_id\"]).batch(1000).as_numpy_iterator()), axis=0)\n",
    "\n",
    "# ScaNN is much more memory efficient and has no problem processing the whole\n",
    "# batch of 20000 queries at once.\n",
    "_, titles = scann(test_flat, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e36144d1-b8d1-487f-93ff-9b0bd142534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(ground_truth, approx_results):\n",
    "  return np.mean([\n",
    "      len(np.intersect1d(truth, approx)) / len(truth)\n",
    "      for truth, approx in zip(ground_truth, approx_results)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f12d886-e887-48cf-a08d-7f3f8e783f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.886\n",
      "11.3 ms ± 22.7 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Recall: {compute_recall(titles_ground_truth, titles):.3f}\")\n",
    "%timeit -n 1000 scann(np.array([\"42\"]), k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c00a12-ffeb-4e7f-9f04-3051c1e6564e",
   "metadata": {},
   "source": [
    "# Different Scann hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27b19e5b-d36e-420e-bc61-b1bfbed16973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 16:26:22.075154: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 100347\n",
      "2024-10-12 16:26:22.105538: W scann/utils/gmm_utils.cc:920] Could not normalize centroid due to zero norm or empty or zero-weight partition.\n",
      "2024-10-12 16:26:22.244429: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:89] PartitionerFactory ran in 169.235865ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.949\n",
      "11.3 ms ± 34.7 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "scann2 = tfrs.layers.factorized_top_k.ScaNN(\n",
    "    model.user_model, \n",
    "    num_leaves=1000,\n",
    "    num_leaves_to_search=100,\n",
    "    num_reordering_candidates=1000)\n",
    "scann2.index_from_dataset(\n",
    "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")\n",
    "\n",
    "_, titles2 = scann2(test_flat, k=10)\n",
    "\n",
    "print(f\"Recall: {compute_recall(titles_ground_truth, titles2):.3f}\")\n",
    "%timeit -n 1000 scann2(np.array([\"42\"]), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71181ebc-19fe-4a17-81ba-3080f19599f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 16:27:54.733708: I scann/partitioning/partitioner_factory_base.cc:59] Size of sampled dataset for training partition: 100347\n",
      "2024-10-12 16:27:54.907381: I ./scann/partitioning/kmeans_tree_partitioner_utils.h:89] PartitionerFactory ran in 173.629372ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.937\n"
     ]
    }
   ],
   "source": [
    "scann3 = tfrs.layers.factorized_top_k.ScaNN(\n",
    "    model.user_model,\n",
    "    num_leaves=1000,\n",
    "    num_leaves_to_search=70,\n",
    "    num_reordering_candidates=400)\n",
    "scann3.index_from_dataset(\n",
    "    tf.data.Dataset.zip((lots_of_movies, lots_of_movies_embeddings))\n",
    ")\n",
    "\n",
    "_, titles3 = scann3(test_flat, k=10)\n",
    "print(f\"Recall: {compute_recall(titles_ground_truth, titles3):.3f}\")\n",
    "%timeit -n 1000 scann3(np.array([\"42\"]), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4832b69-edcb-4b1a-90eb-2195924c0617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
